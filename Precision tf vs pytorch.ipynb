{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f0067ee",
   "metadata": {},
   "source": [
    "# Difference of Convolution Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd412f",
   "metadata": {},
   "source": [
    "I'm feeding a data into a single convolution layer from Pytorch and TensorFlow.\n",
    "I could make sure that:\n",
    "1. the input are the same, they are initialized randomly\n",
    "2. the kernel weights and bias are same, they are initialized randomly\n",
    "3. the way of padding is forced to be the same\n",
    "\n",
    "However, the result shows that there been huge precision differences between the two frameworks. The difference is bigger than 1e-3 and I would be really appreciate it someone could tell me what goes wrong with that. Thanks in advance.\n",
    "\n",
    "Framework Version:\n",
    "1.\tPytorch 1.10.0a0+git593e8f4\n",
    "2.\tTensorFlow 2.5.0 (but itâ€™s tensorflow.compat.v1 in the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7552d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.compat.v1.keras.initializers import he_uniform\n",
    "from tensorflow.compat.v1.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D\n",
    ")\n",
    "from tensorflow.compat.v1.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb34fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63009694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel and input parameters\n",
    "in_ch = 256 # kernel input channels\n",
    "out_ch = 512 # kernel output channels\n",
    "input_shape = [16, 32, in_ch] # input data shape, [h,w,c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c77c3",
   "metadata": {},
   "source": [
    "## case 1: random init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "104d78da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using random number to initialise\n",
    "# unify kernel weights, size k=5\n",
    "init_kernel = np.random.rand(*(5,5,in_ch,out_ch))\n",
    "init_bias = np.random.rand(*(out_ch,))\n",
    "\n",
    "torch_kernel = np.transpose(init_kernel, axes=(3,2,0,1))\n",
    "\n",
    "# unify input data\n",
    "np_input = np.random.rand(*input_shape)\n",
    "tf_input = np.expand_dims(np_input, axis=(0))\n",
    "\n",
    "torch_input = np.expand_dims(np_input, axis=(0))\n",
    "torch_input = np.transpose(torch_input, axes=(0,3,1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d49cda0",
   "metadata": {},
   "source": [
    "## output in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d87943",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8, 16, 512)\n",
      "[[1036.6389  1286.9072  1298.8225  1283.5452  1276.365   1277.3307\n",
      "  1258.4304  1287.5437  1281.8002  1292.6559  1293.8861  1309.453\n",
      "  1286.7378  1277.0677  1301.7002   775.3595 ]\n",
      " [1289.4779  1627.7542  1612.5903  1609.3292  1602.3656  1583.5837\n",
      "  1590.3345  1616.9135  1616.2144  1616.5071  1618.2323  1635.531\n",
      "  1621.8698  1600.3881  1616.7415   959.61127]\n",
      " [1295.3243  1640.8416  1627.5337  1605.8823  1600.4493  1610.625\n",
      "  1611.7426  1625.8309  1637.1473  1613.0815  1625.7319  1628.7733\n",
      "  1616.8772  1612.8221  1619.5997   972.6801 ]\n",
      " [1297.9673  1633.0712  1618.3251  1599.8452  1596.4957  1620.6074\n",
      "  1648.3531  1618.8705  1632.3997  1611.8512  1615.3243  1611.0486\n",
      "  1606.9712  1622.3531  1633.6322   962.0845 ]\n",
      " [1280.9043  1625.0216  1627.7568  1622.6278  1623.4135  1627.2576\n",
      "  1629.244   1629.7815  1629.1494  1610.7152  1592.1129  1593.6815\n",
      "  1603.803   1636.3857  1630.783    965.37335]\n",
      " [1282.802   1621.4127  1618.3734  1641.9825  1606.8539  1611.0704\n",
      "  1604.6492  1622.0945  1611.9672  1604.553   1606.137   1612.1741\n",
      "  1623.7803  1624.1415  1629.6722   956.13525]\n",
      " [1287.735   1615.6346  1622.7924  1638.6718  1585.0696  1605.0867\n",
      "  1623.0238  1623.3173  1623.7715  1622.0089  1600.1842  1602.7664\n",
      "  1599.5398  1587.7744  1585.9847   937.18933]\n",
      " [ 785.1776   975.7148   988.37836  984.1436   962.94403  976.71454\n",
      "   975.3793   964.7883   965.1806   978.7293   975.8232   971.07336\n",
      "   962.422    960.46594  958.8645   584.59296]]\n"
     ]
    }
   ],
   "source": [
    "# tf\n",
    "# assign kernel with predefined weights\n",
    "kernel_initializer = tf.keras.initializers.constant(init_kernel)\n",
    "bias_initializer = tf.keras.initializers.constant(init_bias)\n",
    "inp = Input(shape=input_shape)\n",
    "oup = Conv2D(filters=out_ch, kernel_size=5, strides=(2, 2),kernel_initializer=kernel_initializer,bias_initializer=bias_initializer, padding=\"same\")(inp)\n",
    "model = Model(inp, oup)\n",
    "\n",
    "# forward\n",
    "y = model.predict(x=tf_input)\n",
    "print(y.shape)\n",
    "print(y[0,:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f8662",
   "metadata": {},
   "source": [
    "## output in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd1b29b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 8, 16])\n",
      "[[1036.64335161 1286.9226207  1298.83164532 1283.54729802 1276.36572442\n",
      "  1277.34366467 1258.44408343 1287.55955293 1281.80383857 1292.66285082\n",
      "  1293.8972526  1309.46106419 1286.74941242 1277.07496218 1301.71036886\n",
      "   775.36394238]\n",
      " [1289.49683434 1627.77819436 1612.60660164 1609.34236299 1602.37233848\n",
      "  1583.60523012 1590.35812777 1616.94000353 1616.23318025 1616.52659686\n",
      "  1618.25492932 1635.55053228 1621.88510246 1600.4047178  1616.75682842\n",
      "   959.62300385]\n",
      " [1295.35066291 1640.87627539 1627.55499845 1605.89561287 1600.47077093\n",
      "  1610.65044331 1611.76687007 1625.86000189 1637.17464345 1613.10641884\n",
      "  1625.74788237 1628.78212416 1616.89569293 1612.83844215 1619.61461932\n",
      "   972.68669719]\n",
      " [1297.98683996 1633.09091837 1618.34323033 1599.86776552 1596.51702622\n",
      "  1620.6319902  1648.38013553 1618.88995459 1632.42613947 1611.86861284\n",
      "  1615.34236042 1611.06178193 1606.98307877 1622.37315644 1633.65035889\n",
      "   962.09070446]\n",
      " [1280.92282653 1625.04752549 1627.77473861 1622.65477543 1623.42812271\n",
      "  1627.27969823 1629.26493226 1629.79789296 1629.169639   1610.72149722\n",
      "  1592.11380718 1593.69902183 1603.8145987  1636.40580604 1630.8047341\n",
      "   965.37430628]\n",
      " [1282.80261303 1621.42874663 1618.39970331 1642.0078164  1606.86322523\n",
      "  1611.08294603 1604.66649989 1622.10508734 1611.98763751 1604.56987063\n",
      "  1606.15216581 1612.18938142 1623.80110023 1624.15771072 1629.6917714\n",
      "   956.13762876]\n",
      " [1287.73735359 1615.65325546 1622.82600489 1638.69795674 1585.08812598\n",
      "  1605.10045237 1623.0375213  1623.3359328  1623.79671797 1622.02730622\n",
      "  1600.20018208 1602.7866405  1599.55831522 1587.79420464 1586.01114795\n",
      "   937.18645116]\n",
      " [ 785.18001171  975.7276995   988.39184377  984.15813493  962.95444168\n",
      "   976.72682936  975.39306635  964.80304284  965.18801047  978.74215048\n",
      "   975.83293742  971.08150199  962.43186404  960.46959902  958.8690718\n",
      "   584.59503163]]\n"
     ]
    }
   ],
   "source": [
    "# pytorch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "image = torch.Tensor(torch_input)\n",
    "\n",
    "# pad as tensorflow\n",
    "padding_setting = (1,2,1,2)\n",
    "image = F.pad(image, padding_setting, \"constant\", 0)\n",
    "\n",
    "# assign kernel with predefined weights\n",
    "conv_filter = torch.nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, stride=(2,2), padding=0)\n",
    "conv_filter.weight = torch.nn.Parameter(torch.Tensor(torch_kernel))\n",
    "conv_filter.bias = torch.nn.Parameter(torch.Tensor(init_bias))\n",
    "conv_filter.eval()\n",
    "\n",
    "# forward\n",
    "torch_out = conv_filter(image)\n",
    "print(torch_out.shape)\n",
    "print(torch_out[0,1,...].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbf3e88",
   "metadata": {},
   "source": [
    "## calculate differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c7928a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.transpose(y,axes=(0,3,1,2))\n",
    "t = torch_out.detach().numpy()\n",
    "diff = np.abs(y - t)\n",
    "np.all(diff < 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "842dd7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(diff < 1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eee5da",
   "metadata": {},
   "source": [
    "## another case, simplier but gives True for np.all(diff < 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f8fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using ones and zeros to initialise\n",
    "# unify kernel weights, size k=5\n",
    "init_kernel = np.ones((5,5,in_ch,out_ch))\n",
    "init_bias = np.zeros(*(out_ch,))\n",
    "\n",
    "torch_kernel = np.transpose(init_kernel, axes=(3,2,0,1))\n",
    "\n",
    "# unify input data\n",
    "np_input = np.ones(input_shape)\n",
    "tf_input = np.expand_dims(np_input, axis=(0))\n",
    "\n",
    "torch_input = np.expand_dims(np_input, axis=(0))\n",
    "torch_input = np.transpose(torch_input, axes=(0,3,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4845fe3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
